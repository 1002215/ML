{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Titanic.py\n",
      "./Project_1.ipynb\n",
      "./.ipynb_checkpoints/README-checkpoint.md\n",
      "./.ipynb_checkpoints/Project_1-checkpoint.ipynb\n",
      "./.ipynb_checkpoints/Titanic-checkpoint.py\n",
      "./ML-and-AI/.git/description\n",
      "./ML-and-AI/.git/HEAD\n",
      "./ML-and-AI/.git/config\n",
      "./ML-and-AI/.git/info/exclude\n",
      "./ML-and-AI/.git/hooks/applypatch-msg.sample\n",
      "./ML-and-AI/.git/hooks/commit-msg.sample\n",
      "./ML-and-AI/.git/hooks/pre-applypatch.sample\n",
      "./ML-and-AI/.git/hooks/pre-receive.sample\n",
      "./ML-and-AI/.git/hooks/pre-push.sample\n",
      "./ML-and-AI/.git/hooks/post-update.sample\n",
      "./ML-and-AI/.git/hooks/update.sample\n",
      "./ML-and-AI/.git/hooks/prepare-commit-msg.sample\n",
      "./ML-and-AI/.git/hooks/pre-commit.sample\n",
      "./ML-and-AI/.git/hooks/pre-rebase.sample\n",
      "./ML-and-AI/.git/hooks/push-to-checkout.sample\n",
      "./ML-and-AI/.git/hooks/fsmonitor-watchman.sample\n",
      "./ML-and-AI/.git/hooks/pre-merge-commit.sample\n",
      "./.git/description\n",
      "./.git/HEAD\n",
      "./.git/config\n",
      "./.git/index\n",
      "./.git/objects/29/e8ad9667f8395a9621d104b0bc4f2e67b9e17f\n",
      "./.git/objects/e1/dbcbc8f96e2bd1ba6883b054b5587389ed772f\n",
      "./.git/objects/04/da08463299c74f850a5860ada9be5b76ab205a\n",
      "./.git/objects/c4/a5408d5d3285eb7ebaf392ec1caab929d3e2a1\n",
      "./.git/info/exclude\n",
      "./.git/hooks/applypatch-msg.sample\n",
      "./.git/hooks/commit-msg.sample\n",
      "./.git/hooks/pre-applypatch.sample\n",
      "./.git/hooks/pre-receive.sample\n",
      "./.git/hooks/pre-push.sample\n",
      "./.git/hooks/post-update.sample\n",
      "./.git/hooks/update.sample\n",
      "./.git/hooks/prepare-commit-msg.sample\n",
      "./.git/hooks/pre-commit.sample\n",
      "./.git/hooks/pre-rebase.sample\n",
      "./.git/hooks/push-to-checkout.sample\n",
      "./.git/hooks/fsmonitor-watchman.sample\n",
      "./.git/hooks/pre-merge-commit.sample\n",
      "./data/titanic.csv\n",
      "./data/.ipynb_checkpoints/titanic-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"./\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this porject, you must predict what features contributed the most the passenger's\n",
    "survival on the Titanic and create an ML model predicting the survivability of the\n",
    "passenger. The program must be written in Python and use Kaggle's\n",
    "Titanic - Machine Learning from Disaster datasets.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" I received this data from the Kaggle Titanic - Machine Learning from Disaster training\n",
    "data set called titanic.csv.\"\"\"\n",
    "df = pd.read_csv('data/titanic.csv', skiprows=2)\n",
    "df.columns = ['PassengerId', 'Survival', 'Pclass', 'Name', 'Sex', 'Age', \n",
    "              'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I believe some of the data columns are not helpful to determining what contributed to their\\nsurvival, and as such can be eleimnated to improve run time and performance. My bias is that I\\nbelieve women and children should have been prioritized at the time of the sinking.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"I believe some of the data columns are not helpful to determining what contributed to their\n",
    "survival, and as such can be eleimnated to improve run time and performance. My bias is that I\n",
    "believe women and children should have been prioritized at the time of the sinking.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survival  Pclass  Sex   Age  SibSp  Parch\n",
      "0         1       3    1  26.0      0      0\n",
      "1         1       1    1  35.0      1      0\n",
      "2         0       3    0  35.0      0      0\n",
      "4         0       1    0  54.0      0      0\n",
      "5         0       3    0   2.0      3      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1355462/564097973.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Sex']=df['Sex'].replace({'female': 1, 'male': 0})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# These columns are nonessential and as such can be removed to increase performance and runtime.\n",
    "df = df.drop(columns=['PassengerId','Name','Ticket','Fare','Cabin','Embarked'])\n",
    "\n",
    "# Dropping all rows with none value as it can skew performance\n",
    "df = df.dropna()\n",
    "\n",
    "# Trying to hot encode since female and male are strings and this type\n",
    "# cannot be supported\n",
    "df['Sex']=df['Sex'].replace({'female': 1, 'male': 0})\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9397590361445783\n",
      "Test accuracy: 0.8271028037383178\n",
      "\n",
      "   Feature  Importance\n",
      "2     Age    0.421313\n",
      "1     Sex    0.302996\n",
      "0  Pclass    0.154691\n",
      "3   SibSp    0.068679\n",
      "4   Parch    0.052321\n"
     ]
    }
   ],
   "source": [
    "target = 'Survival'\n",
    "features = df.columns[df.columns!=target]\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=123)\n",
    "\n",
    "# Chose Random Forest Classifier because features and survivability\n",
    "# are nonlinear and this can support, also it\n",
    "# can provide feature importance which is what we need to determine\n",
    "# the most crucial factor in survival.\n",
    "forest = RandomForestClassifier(\n",
    "    n_estimators = 1000,\n",
    "    criterion='gini',\n",
    "    random_state=1,\n",
    "    n_jobs=-1)\n",
    "forest.fit(X_train,y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred = forest.predict(X_test)\n",
    "\n",
    "#about 94% training accuract 83% test accuracy\n",
    "print(\"Train accuracy:\", accuracy_score(y_train,y_train_pred))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "# Age is the most important feature\n",
    "feature_importances = forest.feature_importances_\n",
    "features_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\n\",features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infrence(prams):\n",
    "    results = m.run(prams)\n",
    "    return results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
